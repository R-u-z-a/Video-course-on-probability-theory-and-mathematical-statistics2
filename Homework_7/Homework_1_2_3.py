# Задане № 1

# Дана матрица объект-признак

# X = [[ 1.22401313,  2.30868478,  3.03636353,  2.69287214],
#      [-0.18757272,  1.30337355,  5.12093014,  3.46363202],
#      [-0.81094525,  1.82463398,  5.79686488,  1.86159445],
#      [ 0.75129018,  2.67392052,  3.65529809,  1.66746094],
#      [ 0.00972362,  1.97367255,  2.50594319,  1.69755173],
#      [-0.62972637,  0.77750764,  2.84124027,  4.54410559],
#      [ 2.29536229,  1.81206697,  1.95026215,  1.51874636],
#      [ 0.0920418 ,  2.26971361,  7.47708735,  2.61081203],
#      [ 2.39252799,  3.17563985,  3.61420599,  5.10773362],
#      [ 0.54983815,  2.87988651,  1.65752765,  1.59635987]]

#  и значения целевой переменной

# y = [ 9.26193358,  9.700363  ,  8.67214805,  8.74796974,  6.18689108,
#       7.53312713,  7.57643777, 12.44965478, 14.29010746,  6.68361218]

# Подберите два признака (из четырёх) так, чтобы уровень линейной зависимости целевой переменной от значений этих признаков был максимальным. Другими словами, модель линейной регрессии на этих признаках должна давать наилучший результат. В качестве ответа запишите значение коэффициента детерминации для модели на этих признаках.

# Решение

import numpy as np
from itertools import combinations
from scipy import stats

X = [[1.22401313, 2.30868478, 3.03636353, 2.69287214],
     [-0.18757272, 1.30337355, 5.12093014, 3.46363202],
     [-0.81094525, 1.82463398, 5.79686488, 1.86159445],
     [0.75129018, 2.67392052, 3.65529809, 1.66746094],
     [0.00972362, 1.97367255, 2.50594319, 1.69755173],
     [-0.62972637, 0.77750764, 2.84124027, 4.54410559],
     [2.29536229, 1.81206697, 1.95026215, 1.51874636],
     [0.0920418, 2.26971361, 7.47708735, 2.61081203],
     [2.39252799, 3.17563985, 3.61420599, 5.10773362],
     [0.54983815, 2.87988651, 1.65752765, 1.59635987]]

X = np.array(X)

y = [9.26193358, 9.700363, 8.67214805, 8.74796974, 6.18689108,
     7.53312713, 7.57643777, 12.44965478, 14.29010746, 6.68361218]
y = np.array(y)

print(X.shape, y.shape)


def sum_of_squares(samples: np.ndarray) -> float:
    """Сумма квадратов отклонений."""
    return ((samples - samples.mean()) ** 2).sum()


def get_determination_coefficient(x: np.ndarray, y: np.ndarray) -> float:
    """Эта функция строит по матрице объект-признак `x` и массиву значений
    целевой переменной `y` модель линейной регрессии и возвращает её
    коэффициент детерминации."""

    ones = np.ones((x.shape[0], 1))
    x = np.hstack([ones, x])

    xtx = x.T.dot(x)
    xtx_inv = np.linalg.inv(xtx)

    b = xtx_inv.dot(x.T).dot(y)

    z = x.dot(b)
    E = y - z

    return 1 - sum_of_squares(E) / sum_of_squares(y)


for i, j in combinations(range(X.shape[1]), 2):
    r = get_determination_coefficient(X[:, [i, j]], y)

    print(f'{i} {j} {r}')

k1 = 2
k2 = X.shape[0] - k1 - 1

R = get_determination_coefficient(X[:, [0, 2]], y)
F = (R / k1) / ((1 - R) / k2)
print(F)

alpha = 0.05

t = stats.f.ppf(1 - alpha, k1, k2)
print(t)

# Значение статистики попало в критическую область, значит,
# уравнение регрессии признаётся статистически значимым.

# Задание № 2

# Для проведения A/B-тестирования сайта интернет-магазина были получены следующие данные: страница A была посещена 2509 раз, из них 77 закончились совершением покупки, страница B была посещена 1465 раз, 60 из них закончились совершением покупки.
# Является ли значимым отличие конверсии на страницах A и B?
# Подсказка. Реализуйте двухвыборочный t-тест.
# В качестве выборок здесь можно взять наборы меток совершения покупки (0 или 1) каждым посетителем.

# Решение

# Выборки


x1 = np.zeros(2509)
x1[np.arange(77)] = 1

x2 = np.zeros(1465)
x2[np.arange(60)] = 1

# двухвыборочный t-тест.

n1 = x1.size
n2 = x2.size

s1 = x1.std(ddof=1)
s2 = x2.std(ddof=1)

s_delta = np.sqrt(s1 ** 2 / n1 + s2 ** 2 / n2)
print(s_delta)

t = (x1.mean() - x2.mean()) / s_delta
print(t)

# Посчитаем число степиней

df = (s1 ** 2 / n1 + s2 ** 2 / n2) ** 2 / ((s1 ** 2 / n1) ** 2 / (n1 - 1) + (s2 ** 2 / n2) ** 2 / (n2 - 1))
print(df)

# Возьмём уровень значимости

alpha = 0.05

t1 = stats.t.ppf(alpha / 2, df=df)
t2 = stats.t.ppf(1 - alpha / 2, df=df)
print(t1, t2)

# Задача № 3

# Квартет Энскомба — популярный в области анализа данных пример наборов данных, у которых практически совпадают все статистические свойства (средние, дисперсии, коэффициенты корреляции, регрессионные линии), однако, существенно отличаются графики.
# Данный пример призван показать, насколько важна визуализация данных.
# Датасет представляет собой 4 пары выборок:
# {
#     "x1": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
#     "y1": [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],
#     "x2": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
#     "y2": [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74],
#     "x3": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
#     "y3": [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],
#     "x4": [8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0],
#     "y4": [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89]
# }
# По каждой паре выборок посчитайте:
# выборочное среднее и дисперсию каждой выборки,
# коэффициент корреляции Пирсона и прямую линейной регрессии.
# Убедившись в том, что они практически не отличаются, постройте scatter plot по каждой паре выборок.

# Решение

# Загружаем датасет

data = {
    "x1": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
    "y1": [8.04, 6.95, 7.58, 8.81, 8.33, 9.96, 7.24, 4.26, 10.84, 4.82, 5.68],
    "x2": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
    "y2": [9.14, 8.14, 8.74, 8.77, 9.26, 8.1, 6.13, 3.1, 9.13, 7.26, 4.74],
    "x3": [10.0, 8.0, 13.0, 9.0, 11.0, 14.0, 6.0, 4.0, 12.0, 7.0, 5.0],
    "y3": [7.46, 6.77, 12.74, 7.11, 7.81, 8.84, 6.08, 5.39, 8.15, 6.42, 5.73],
    "x4": [8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 8.0, 19.0, 8.0, 8.0, 8.0],
    "y4": [6.58, 5.76, 7.71, 8.84, 8.47, 7.04, 5.25, 12.5, 5.56, 7.91, 6.89]
}

for i in range(1, 5):
    x = data[f'x{i}']
    y = data[f'y{i}']

    print(f'Выборки {i}')
    print(f'Выборочные средние: {np.mean(x)}, {np.mean(y)}')
    print(f'Выборочные дисперсии: {np.var(x)}, {np.var(y)}')
    print(f'Коэффициент корреляции: {np.corrcoef(x, y)[0, 1]}\n')

from matplotlib import pyplot as plt

plt.style.use('seaborn-whitegrid')

fig, axes = plt.subplots(nrows=2, ncols=2)
fig.set_size_inches(10, 6)

ox = np.linspace(3, 20, 10 ** 4)

for i, ax in enumerate(axes.flatten()):
    i += 1

    x = data[f'x{i}']
    y = data[f'y{i}']

    b1 = np.cov(x, y, ddof=0)[0, 1] / np.var(x, ddof=0)
    b0 = np.mean(y) - b1 * np.mean(x)

    print(f'Выборки {i}: b0 = {round(b0, 4)},\tb1 = {round(b1, 4)}')

    oy = b0 + b1 * ox

    ax.scatter(x, y)
    ax.plot(ox, oy, color='red', alpha=0.5)

    ax.set_xlim(3, 20)
    ax.set_ylim(2, 14)

    ax.set_title(f'Выборки {i}')

print()
